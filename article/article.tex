\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[a4paper, total={16cm, 23cm}]{geometry}
\usepackage{setspace}
\usepackage{bbold}
\usepackage{chngcntr}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=red,
	pdftitle={Rapport de Stage},
	pdfauthor={Etienne MARION},
	pdfpagemode=FullScreen,
}

\newcommand{\lean}[1]{\lstinline[language=lean]{#1}}

\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1}   % red
\definecolor{tacticcolor}{rgb}{0.0, 0.1, 0.3}    % dark blue
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{stringcolor}{rgb}{0.5, 0.3, 0.2}    % brown
\definecolor{symbolcolor}{rgb}{0.1, 0.2, 0.7}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green
\definecolor{attributecolor}{rgb}{0.7, 0.1, 0.1} % red
\definecolor{errorcolor}{rgb}{1, 0, 0}           % bright red

\usepackage{listings}
\def\lstlanguagefiles{lstlean.tex}
\lstloadlanguages{lean}
\lstset{language=lean}
\usepackage{scalefnt}

\newcommand{\sorry}[0]{\lean{sorry}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\imp}{\Rightarrow}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\inv}{^{-1}}
\newcommand{\eps}{\epsilon}
\newcommand{\type}{\texttt{Type}}
\newcommand{\prop}{\texttt{Prop}}
\newcommand{\prun}{\texttt{pr}_1}
\newcommand{\prde}{\texttt{pr}_2}
\newcommand{\inl}{\texttt{inl}}
\newcommand{\inr}{\texttt{inr}}
\newcommand{\skl}{\vspace{\baselineskip}}
\newcommand{\gui}[1]{\og{}#1\fg{}}
\newcommand{\Xge}[1]{X^{\ge#1}}
\newcommand{\Xle}[1]{X^{\le#1}}
\newcommand{\Xgt}[1]{X^{>#1}}
\newcommand{\Age}[1]{\mathcal{A}^{\ge#1}}
\newcommand{\Agt}[1]{\mathcal{A}^{>#1}}
\newcommand{\Ale}[1]{\mathcal{A}^{\le#1}}
\newcommand{\Xgtn}{X^{>n}}
\newcommand{\Xgtm}{X^{>m}}
\newcommand{\Xgtk}{X^{>k}}
\newcommand{\Xlen}{X^{\le n}}
\newcommand{\Alen}{\mathcal{A}^{\le n}}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\dy}{\mathrm{d}y}
\newcommand{\prth}[1]{\left(#1\right)}
\renewcommand{\empty}{\varnothing}
\newcommand{\tendl}[1]{\longrightarrow_{#1}}
\newcommand{\dbrack}[1]{\llbracket #1 \rrbracket}
\newcommand{\priv}{\,\backslash\,}
\newcommand{\restr}[1]{\vert_{#1}}
\newcommand{\mub}{\overline{\mu}}
\newcommand{\ox}{\otimes}
\renewcommand{\phi}{\varphi}
\newcommand{\piun}{{\pi_{\dbrack{1,n}}}}
\newcommand{\pizn}{{\pi_{\dbrack{0,n}}}}
\newcommand{\Xint}[1]{X^{\dbrack{#1}}}
\newcommand{\tost}{\to_*}
\newcommand{\ind}[1]{\mathbb{1}_{#1}}
\newcommand{\dmu}{\mathrm{d}\mu}
\newcommand{\dlamb}{\mathrm{d}\lambda}
\newcommand{\mathlib}{\texttt{mathlib}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[thm]
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{defi}[thm]{Definition}
\newtheorem{eg}[thm]{Example}
\theoremstyle{remark}
\newtheorem*{nota}{Notation}

\begin{document}

	\begin{center}
		{\Large\bf A Formalization of the Ionescu-Tulcea Theorem in mathlib} \\
		\vspace{1cm}
		Author : Etienne Marion \\
		\vspace{1cm}
		ENS de Lyon, 46, Allée d’Italie, 69007 Lyon, France \\
		\texttt{etienne.marion@ens-lyon.fr}
	\end{center}

	\vspace{0.5cm}

	\section*{Abstract}

	\section{Introduction}
	Being able to talk about the joint distribution of an infinite family of random variables is crucial in probability theory. For example, one often requires a family of independent random variables. The existence of such a family relies on the existence of an infinite product measure. Indeed, given $(\Omega_i, \F_i, \mu_i)_{i\in\iota}$ a family of probability spaces, the existence of the product measure $\bigotimes_{i\in\iota}\mu_i$ yields a new probability space $(\prod_{i\in\iota}\Omega_i, \bigotimes_{i\in\iota}\F_i, \bigotimes_{i\in\iota}\mu_i)$, and the projections $X_i : \prod_{j\in\iota}\Omega_j \to \Omega_i$ give the desired family. For another example, consider discrete-time Markov chains. Given a measurable space $(E, \A)$ and a Markov kernel $\kappa : E \to E$, one might want to build a sequence $(X_n)_{n\in\N}$ of random variables with values in $E$ such that the conditional distribution of $X_{n+1}$ given $X_0, ..., X_n$ is $\kappa(X_n,\cdot)$. Such objects are fundamental in probability theory: families of independent variables allow to build more complicated objects, such as Brownian motion, while discrete-time Markov chains form a huge class of stochastic processes which contains random walks for instance. It so happens that those objects always exist without any restrictions on the spaces we consider. This is a direct consequence of the Ionescu-Tulcea theorem.

	The goal of this contribution is to provide a formalization of the proof of the aforementioned theorem using the proof assistant Lean and the associated library \mathlib. It heavily relies on previous work done in [cite] which formalizes the Kolmogorov extension theorem. While this specific theorem is not used directly in the formalization presented here, we use the notion of measurable cylinders, projective families or additive contents which were introduced to prove the Kolmogorov extension theorem.

	\section{Statement of the theorem}
	The Ionescu-Tulcea theorem is about the existence of a certain Markov kernel. We therefore start with a reminder about those.

	\begin{defi}\label{def:markov-kernel}
		Let $(X,\A)$ and $(Y,\B)$ be two measurable spaces. A \emph{Markov kernel} from $X$ to $Y$ is a map $\kappa : X \times \B \to [0,1]$ such that:
		\begin{itemize}
			\item for any $x \in X$, $\kappa(x,\cdot)$ is a probability measure;
			\item for any $B \in \B$, $\kappa(\cdot,B)$ is measurable.
		\end{itemize}
		One can therefore consider a Markov kernel as a measurable map which to any point in $X$ associates a probability distribution over the space $Y$.
	\end{defi}

	To ease the writing, let us give some notations.

	\begin{nota}
		If $I$ is a set of indices and $(X_i, \A_i)_{i \in I}$ a family of measurable spaces, then:
		\begin{itemize}
			\setstretch{1.5}
			\item we write $X$ for $\prod_{i \in I} X_i$ and $\A$ for $\bigotimes_{i \in I} \A_i$;
			\item if $J \subseteq I$, we write $X^J$ for $\prod_{i \in J} X_i$ and $\A^J$ for $\bigotimes_{i \in J} \A_i$;
			\item if $I = \N$ and $n \in \N$ we write $\Xge{n}$ for $X^{\set{k \in \N \mid k \ge n}}$ and $\Age{n}$ for $\A^{\set{k \in \N \mid k \ge n}}$, with similar notations for $\le, >, <$;
			\item if $J \subseteq I$, we write $\pi_J$ for the canonical map from $X$ to $X^J$;
			\item if $ K \subseteq J \subseteq I$, we write $\pi_{J\to K}$ for the canonical map from $X^J$ to $X^K$.
		\end{itemize}
	\end{nota}

	Let us now state the theorem.

	\begin{thm}[Ionescu-Tulcea]\label{thm:it}
		Let $(X_n,\A_n)_{n\in\N}$ be a family of measurable spaces. Let $(\kappa_n)_{n\in\N}$ be a family of Markov kernels such that for any $n$, $\kappa_n$ is a kernel from $X_0 \times ... \times X_n$ to $X_{n+1}$. Then there exists a unique Markov kernel $\eta : X_0 \to \prod_{n\ge1}X_n$ such that for any $n\ge1$,
		$${\pi_n}_*\eta = \kappa_0 \otimes ... \otimes \kappa_{n-1}.$$
	\end{thm}

	By taking each kernel to be constant we easily get the product of a countable family of probability measures, which can then be used to build the product of an arbitrary family of measures. Likewise if we take $X_n = X$ for all $n$ and $\kappa_n = \kappa$ a kernel from $X \to X$ we get a Markov kernel from $X$ to $X^{\N_+}$ which, if we compose it with a measure $\mu$ at the beginning, gives the measure associated to a Markov chain with initial distribution $\mu$ and transition kernel $\kappa$.

	We will now give a sketch of the proof of the theorem to highlight the elements which are required for the formalization. The idea is to use the Carathéodory extension theorem which allows to extend a measure once we have defined it on a restricted family of measurable sets for which it is to give the expression of the measure. In what follows we consider $(X_n, \A_n)_{n\in\N}$ a family of measurable spaces and we set $\pi_n := \piun$. we also consider of Markov kernels $(\kappa_n : \Xlen \tost X_{n+1})_{n\in\N}$ and we set $\eta_{k,n} := \kappa_k \ox ... \ox \kappa_{n-1} : \Xle{k} \tost \Xint{k+1,n}$ and $\eta_n := \eta_{0,n}$.

	\begin{defi}
		\label{def:cylindres}
		Sets of the form $A \times \Xgtn$, where $n \ge 1$ and $A \in \Alen$ are called cylinders.
	\end{defi}

	By writing $A \times \Xgtn = \pi_n\inv(A)$ it is easy to check that the set of cylinders is closed by union and complement, and thus it is an algebra of sets which generates the product $\sigma$-algebra. We now have to define a measure over cylinders which we will then extend to the whole $\sigma$-algebra. We also do this for cylinders in $\Xge{n}$ for $n\ge1$ as we will need it for the extension afterwards.

	\begin{defi}
		\label{def:content}
		Let $x_0 \in X_0$. If $A := A' \times \Xgtn \subseteq \Xge{1}$ is a cylinder we set
		$$P_0(x_0, A) := \eta_n(x_0, A').$$
		This map is well defined because if $n \le m$ then
		\begin{align*}
			\eta_m\prth{x_0, A' \times \Xint{n+1,m}} &= \eta_n \ox \eta_{n+1,m} \prth{x_0, A' \times \Xint{n+1,m}} \\
			&= \int_{A'} \eta_{n+1, m}\prth{x_0, ..., x_n, \Xint{n+1,m}} \eta_n(x_0, \dx_1...\dx_n) \\
			&= \int \ind{A'} \eta_n(x_0, \dx_1...\dx_n) \\
			&= \eta_n(x_0, A').
		\end{align*}

		For $k \ge 1$ and $A := A' \times \Xgtn \subseteq \Xgt k$,we define likewise
		$$P_k (x_0, ..., x_k, A) := \eta_{k,n} (x_0,...,x_k,A').$$
	\end{defi}

	\begin{lem}
		\label{lem:add-cont}
		For any $k \in \N$ and any $(x_0,...,x_k) \in \Xle{k}$, the map $P_k(x_0,...,x_k, \cdot)$ is an additive content over the cylinders. Namely if $A, B$ are two disjoint cylinders then $P_0(x_0,...,x_k,A \cup B) = P_0(x_0,...,x_k,A) + P_0(x_0,...,x_k,B)$.
	\end{lem}

	\begin{proof}
		We only give the proof in the case $k=0$ as the other cases are identical. Let $A := A' \times \Xgtm$ and $B := B' \times \Xgtn$ be two disjoint cylinders and assume without loss of generality that $m \le n$. We then set $A'' := A' \times \Xint{m+1,n}$. As $A \cap B = \empty$ this implies $A'' \cap B' = \empty$. Moreover, $A \cup B = (A'' \cup B') \times \Xgtn$. Consequently :
		$$P_0(x_0,A\cup B) = \eta_n(x_0,A'' \cup B') = \eta_n(x_0,A'') + \eta_n(x_0,B') = P_0(x_0,A) + P_0(x_0,B).\eqno\mbox{\qedhere}$$
	\end{proof}

	To apply Carathéodory theorem it remains to prove that $P_0(x_0,\cdot)$ is $\sigma$-additive over cylinders. Because it is an additive content and cylinders are an algebra of sets it is enough to show the following result:

	\begin{lem}
		\label{lem:tendsto_zero}
		Let $x_0 \in X_0$. If $(A_n)_{n\in\N}$ is a non-increasing sequence of cylinders with emty intersection then $P_0(x_0, A_n) \tendl{n\to\infty} 0$.
	\end{lem}

	\begin{proof}
		For any $n$ we write $A_n := B_n \times \Xgt{a_n}$. In this proof if $A \subseteq \Xge{1}$ et $(x_1, ..., x_n) \in \Xint{1,n}$ we will denote by $A(x_1,...,x_n) := \set{y \in \Xgtn : (x_1,...,x_n,y) \in A}$ the associated section of $A$.

		We use contraposition. Assume there exists $\eps>0$ such that for any $n \in \N$, $P_0(x_0,A_n) \ge \eps$. We wish to prove that the intersection of $A_n$ is nonempty. We define the following sequence of functions:
		\begin{align*}
			f_n : X_1 &\to [0, 1] \\
			x_1 &\mapsto P_1(x_0,x_1, A_n(x_1)).
		\end{align*}
		It allows us to write
		\begin{align*}
			P_0(x_0,A_n) &= \eta_{a_n}(x_0,B_n) \\
			&=\kappa_0 \ox \eta_{1, a_n}(B_n) \\
			&= \int_{X_1} \eta_{1, a_n}(x_0, x_1, B_n(x_1)) \kappa_0(x_0, \dx_1) \\
			&= \int_{X_1} P_1(x_0,x_1,A_n(x_1)) \kappa_0(x_0, \dx_1) \\
			&= \int_{X_1} f_n(x_1) \kappa_0(x_0, \dx_1).
		\end{align*}
		Because $(A_n)$ is non-increasing it is also the case of $(f_n)$. We set $g := \lim_{n\to\infty} f_n$. By dominated convergence theorem we get $\eps \le P_0(x_0,A_n) \tendl{n\to\infty} \int_{X_1} g(x_1) \kappa_0(x_0,\dx_1)$. As $\kappa_0(x_0, \cdot)$ is a probability measure there exists $x_1 \in X_1$ such that $g(x_1) \ge \eps$, and so for any $n\in\N$, $f_n(x_1) \ge \eps$ which implies that $P_1(x_0,x_1, A_n(x_1)) \ge \eps$.

		We can thus repeat the same step replacing $P_0(x_0,\cdot)$ by $P_1(x_0,x_1,\cdot)$ and $A_n$ by $A_n(x_1)$. We get a sequence $x := (x_k)_{k\ge1}$ such that for any $n\in\N$ and $k\ge1$ we have $P_k(x_0,...,x_k,A_n(x_1,...,x_k)) \ge \eps$. It remains to show that $x \in \bigcap_{n\in\N}A_n$.

		Let $n\in\N$. As $A_n = B_n \times \Xgt{a_n}$ we in particular have that $A_n(x_1,...,x_{a_n})$is either $\Xgt{a_n}$ or empty according to wether $(x_1,...,x_{a_n})$ belongs to $B_n$ or not. But as $P_{a_n}(x_0,...,x_{a_n},A_n(x_1,...,x_{a_n})) \ge \eps > 0$ we have $A_n(x_1,...,x_{a_n}) = \Xgt{a_n}$, which proves that $x \in A_n$.
	\end{proof}

	This concludes the main argument of the proof. The measurability is easily provided by applying the $\pi$-$\lambda$ theorem. We will now focus on those aspects of the proof which proved difficult to formalize and how they were overcome.

	\section{Dealing with composition of kernels}

	One of the key tools for the Ionescu-Tulcea theorem is to be able to speak about the composition of Markov kernels. Indeed that is necessary even to be able to state the theorem. However formalizing this in a way which allows us to smoothly write products proved to be quite challenging.

	One common rule in formalization is that if we use "..." in a proof it will likely be hard to formalize. In our case the statement of Theorem~\ref{thm:it} itself makes use of these when we write the composition of kernels, and they hide a real problem.

	Transition kernels and their composition are already defined in Lean. However we need to take the composition of a finite family of kernels which does not exist in Mathlib, and here is why. By hand, if we have three Markov kernels $\kappa : X \tost Y$, $\eta : X \times Y \tost Z$ and $\xi : X \times Y \times Z \tost T$ we can define $\kappa \ox \eta \ox \xi := (\kappa \ox \eta) \ox \xi$. Then we easily get $\kappa \ox \eta \ox \xi = \kappa \ox (\eta \ox \xi)$. But here we forget a detail: $X \times Y \times Z$ does not mean anything a priori. We can write $(X \times Y) \times Z$ or $X \times (Y \times Z)$ but those are two different types. In paper maths they are canonically isomorphic and we often act as though they were the same but it Lean this isomorphism has to be made explicit.

	Coming back to kernels I can write that $\xi$ is a kernel from $X \times (Y \times Z)$ to $T$. Then $(\kappa \ox \eta) \ox \xi$ is a kernel from $X$ to $(Y \times Z) \times T$. But in that case $\kappa \ox (\eta \ox \xi)$ is not even well defined because $\xi$ should start from $(X \times Y) \times Z$. If we were to add an isomorphism to define this object anyway the equality $(\kappa \ox \eta) \ox \xi = \kappa \ox (\eta \ox \xi)$ would still make no sense because on the left we have a kernel from $X$ to $(Y \times Z) \times T$ while on the right it is a kernel from $X$ to $Y \times (Z \times T)$, so they do not have the same type.

	A similar issue arises when formalizing Theorem~\ref{thm:it}: if we have two kernels $\kappa : X_0 \tost \Xint{1,n}$ and $\eta : \Xlen \tost \Xint{n+1,m}$, their composition does not make sense, we need isomorphisms. First to see $\eta$ as a kernel from $X_0 \times \Xint{1,n}$ to $\Xint{n+1,m}$ to get $\kappa \ox \eta : X_0 \tost \Xint{1,n} \times \Xint{n+1,m}$, then to see $\kappa \ox \eta$ as a kernel from $X_0$ to $\Xint{1,m}$. To solve this problem we need to make these isomorphisms explicit and then hide them inside a black box to be able to use it without worrying about the implementation.

	This work was initiated by Degenne. He introduced a compositon of kernels indexed by integer intervals which makes the equivalence invisible.
	\begin{lstlisting}
	def compProdNat {i j k : ℕ} (κ : Kernel ((l : Iic i) → X l)
		((l : Ioc i j) → X l))
		(η : Kernel ((l : Iic j) → X l) ((l : Ioc j k) → X l)) :
		Kernel ((l : Iic i) → X l) ((l : Ioc i k) → X l)
	\end{lstlisting}
	This takes two kernels $\kappa : \Xle{i} \tost X^{\dbrack{i+1,j}}$ and $\eta : \Xle{j} \tost X^{\dbrack{j+1,k}}$ and returns a kernel from $\Xle{i}$ to $X^{\dbrack{i+1,k}}$. Degenne then proves that this composition is associative, and uses it to define
	\begin{lstlisting}
	def kerNat (κ : (k : ℕ) → Kernel ((l : Iic k) → X l) (X (k + 1))) (i j : ℕ) :
		Kernel ((l : Iic i) → X l) ((l : Ioc i j) → X l).
	\end{lstlisting}
	This takes as an argument a family of kernels $(\kappa_k)_{k\in\N}$ with $\kappa_k : \Xle{k}\to X_{k+1}$ just as in Theorem~\ref{thm:it}. Then given two integers $i$ and $j$ it returns a kernel \lean{kerNat κ i j} which is the composition
	$$\kappa_i \ox \kappa_{i+1} \ox \cdots \ox \kappa_{j-1}.$$
	We are therefore now able to speak about those compositions which are necessary to state the theorem. However \lean{kerNat} proved to be not enough for later purposes. Indeed after proving the theorem and trying to write some API we thought that it would be best to to have a kernel which does not necesarrily starts at time 0 but at any instant. This  would prove nicer for example when it comes to write formulas for the conditional expectation against the Ionescu-Tulcea kernel (see Section). However we bumped into an issue. Having different kernels starting at different times was proving hard to manipulate even with the compositions that were implemented. That is why I chose to add another layer on top of \lean{kerNat} which proved to work out smoothly.
	\begin{lstlisting}
	def partialKernel (a b : ℕ) :
		Kernel ((i : Iic a) → X i) ((i : Iic b) → X i)
	\end{lstlisting}
	Given integer $a$ and $b$ this function will output a kernel from $\Xle{a}$ to $\Xle{b}$. The idea is that the input of this kernel is the full trajectory up to time $a$ and the output is the distribution of the full trajectory up to time $b$. Of course this distribution is deterministic up to time $a$ and then we compose with \lean{kerNat κ a b}. The advantage of this construction is that in the end we always manipulate kernels with domains and codomains of the form $\Xlen$ for some integer $n$ which makes all the composition much easier. For example if we take $a$, $b$ and $c$ three integers satisfying $a \land c \le b$ we get
	\begin{lstlisting}
	(partialKernel κ b c) ∘ₖ (partialKernel κ a b) = partialKernel κ a c
	\end{lstlisting}
	where $\circ_{\rm k}$ is the composition of kernels. This nice property has the advantage to be expressed with a regular operation on kernels rather than one which would be made up for the occasion. With this tool we get a satisfactory way of expressing the composition of multiple kernels.

	\section{Sections of sets}

	We see that the proof of Lemma~\ref{lem:tendsto_zero} heavily relies on the use of sections of sets. This however is not well suited for formalization purposes because of the following. The set $\prod_{n\in\N} X_n$ is represented in lean as the type of functions $f$ with domain $\N$ and such that for any $n \in \N$ $f(n)$ has type $X_n$. We encounter the same issue as before: $\prod_{n\in\N} X_n$ is different from $X_0 \times \Xge{1}$ so we have to use isomorphisms. Moreover, if $A \subseteq \prod_{n\in\N} X_n$ and $x \in X_0$then $A_x \subseteq \Xge{1}$.
	So $A_x$ is a set of sequences indexed by $\N_+$ which is not a subset of $\N$ but rather a subtype in Lean. This makes things hard to manipulate because we want to deal with an element of $\N_+$ we have to provide a proof that it is positive. This would have to be done not only for non-zero integers but also for integers greater than $2$, $3$ and so on with the same troubles each time. Therefore Sébastien Gouëzel suggested another approach.

	The idea relies on the notion of marginals exposed in Section~4 of [Van2024]. I generalized their idea as follows: given $a$ and $b$ integers, $f : X \to [0,+\infty]$ and $x \in X$ I defined \lean{lamrginalPartialKernel κ a b f x} as
	$$\int_{\Xle{b}} f(y \leadsto_b x) \mathrm{d}\eta_{a,b}(x_0,...,x_a).$$

	This is great because it allows to write $$\texttt{lamrginalPartialKernel $\kappa$ b c (lamrginalPartialKernel $\kappa$ a b f x) x},$$
	in other words we can first integrate between $a$ and $b$ and then between $b$ and $c$. This allows us to peel off the measure of a cylinder as the integral of its indicator:
	$$P_0(x_0, A_n) = \int_{X_1} P_1(x_0, x_1, A_n(x_1)) \kappa_0(x_0,\dx_1).$$

%	In this paper they explain how we can't directly write
%	$$\int_{\R^{n+m}}f(x)\dx=\int_{\R^n}\int_{\R^m}f(x,y)\dx\dy$$
%	because $\R^{n+m}$ and $\R^n\times\R^m$ are not the same type in lean. Therefore given $(X_i, \A_i, \mu_i)_{i\in\iota}$ a family of measure spaces, $I$ a finite subset of $\iota$ and $f : \prod_{i\in\iota} X_i \to [0,+\infty]$,	they define the following function
%	\begin{align*}
%		\int\cdots\int_{i\in I}f\dmu_i : \prod_{i\in\iota}X_i &\to [0,+\infty] \\
%		x &\mapsto \int_{\prod_{i \in I}X_i}f(y \leadsto_I x) \mathrm d \bigotimes_{i\in A}\mu_i(dy),
%	\end{align*}
%	where $y \leadsto_J x$ is defined as
%	$$(y \leadsto_J x)_i :=
%	\begin{cases}
%		y_i, &\text{si $i \in J$} \\
%		x_i, &\text{sinon}
%	\end{cases}.
%	$$
%	What is happening here is that we are computing

	\section{Dependence in coordinates}

	This trick of considering some function as defined over $X$ while it really only depends on the coordinates $\ge n$ brings the necessity of introducing a simple way to indicate which variables a given function actually depends on. We therefore introduced the following predicate.
	\begin{lstlisting}
	def DependsOn (f : ((i : ι) → α i) → β) (s : Set ι) : Prop :=
	∀ ⦃x y⦄, (∀ i ∈ s, x i = y i) → f x = f y
	\end{lstlisting}
	The idea is that if $f : \prod_{i\in\iota} \alpha_i \to \beta$ and $s \subset \iota$, \lean{DependsOn f s} means that to check $f(x) = f(y)$ it is enough to show that for any $i \in s$, $x_i = y_i$. In other words $f$ only depends on coordinates indexed by $s$. For example, if $f$ only depends on coordinates $\le b$ and we integrate over its coordinates between $a$ and $b$ then the output only depends on coordinates $\le a$. This is formalized as follows.
	\begin{lstlisting}
	theorem dependsOn_lmarginalPartialKernel (a : ℕ) {b : ℕ}
		{f : ((n : ℕ) → X n) → ℝ≥0∞}
		(hf : DependsOn f (Iic b)) (mf : Measurable f) :
		DependsOn (lmarginalPartialKernel κ a b f) (Iic a)
	\end{lstlisting}

	\section{Writing an API}

	When formalizing a new mathematical object one usually does not stop at writing the definition. Indeed the objected is intended to be usable by all the users without having to know how the object was implemented. That is why it is important to accompany the object with many results which allow other users to use the object without knowing the implementation details.

	We now present some of these results. The API is still very basic and will be developed as other users pick up the object to formalize more objects. The best way to find new lemmas to formalize is to choose a target statement and try and formalize it. In our case we aim to prove the following result.

	\begin{nota}
		For $n \in \N$ we set $\F_n := \pi_{\dbrack{0,n}}\inv\prth{\Alen}$.
	\end{nota}

	\begin{thm}
		\label{thm:condexp}
		Let $a, b \in \N$ with $a \le b$. Let $u \in \Xle{a}$. Let $f : X \to \R$ be an integrable function with respect to $\lambda_a(u)$. Then for $\lambda_a(u)$-almost every $x := (x_n)_{n\in\N}$, $f$ is integrable with respect to $\lambda_b(x_0,...,x_b)$ and
		$$\E_{\lambda_a(u)}[f \mid \F_n] (x) = \int_X f(y) \lambda_b(x_0,...,x_b, \dy).$$
	\end{thm}

	Let us give the proof to highlight the necessary intermediate results. Take $A \in \Ale{b}$ and set $\pi := \pi_{\dbrack{0,b}}$. Our goal is to prove that
	$$\int_{\pi\inv(A)} \int_X f(y) \lambda_b(\pi(x), \dy)\lambda_a(u,\dx) = \int_{\pi\inv(A)} f \dlamb_a.$$
	We start by rewriting the outer integral using the push-forward measure:
	$$\int_{\pi\inv(A)} \int_X f(y) \lambda_b(\pi(x), \dy)\lambda_a(u,\dx) = \int_A \int_X f(y) \lambda_b(\pi(x), \dy)(\pi_*\lambda_a)(u,\dx).$$
	The first we will need is that $\pi_*\lambda_a = \eta_{a,b}$. This is quite obvious: as $\lambda_a$ gives the whole trajectory, keeping only the cooordinates up to time $b$ yields the trajectory up to time $b$. This is formalized as below.
	\begin{lstlisting}
theorem ionescuTulceaKernel_proj (a b : ℕ) :
(ionescuTulceaKernel κ a).map (frestrictLe b) = partialKernel κ a b
	\end{lstlisting}
	So we get $$\int_{\pi\inv(A)} \int_X f(y) \lambda_b(\pi(x), \dy)\lambda_a(u,\dx) = \int_A \int_X f(y) \lambda_b(\pi(x), \dy)\eta_{a,b}(u,\dx).$$
\end{document}
